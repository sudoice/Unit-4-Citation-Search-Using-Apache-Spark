{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCkL+rBknTkhduHx58d7jJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudoice/Unit-4-Citation-Search-Using-Apache-Spark/blob/main/citation_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkeDkTFcRZur"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes, LinearSVC, GBTClassifier\n",
        "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, MultilayerPerceptronClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col, when, split, udf, abs\n",
        "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StringType\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "import pyspark.sql.functions as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration parameters\n",
        "NODE_INFO_FILENAME = \"node_information.csv\"\n",
        "TRAINING_SET_FILENAME = \"training_set.txt\"\n",
        "TESTING_SET_FILENAME = \"testing_set.txt\"\n",
        "GROUND_TRUTH_FILENAME = \"Cit-HepTh.txt\"\n",
        "INFO_DATAFRAME_PORTION = 0.1\n",
        "LOGISTIC_REGRESSION_ITERATIONS = 100"
      ],
      "metadata": {
        "id": "VsJbPXtvRhVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_publication_year_difference(year_from, year_to):\n",
        "    \"\"\"Calculate absolute difference between publication years\"\"\"\n",
        "    if year_from is None or year_to is None:\n",
        "        return 0\n",
        "    try:\n",
        "        # Convert string values to integers before subtraction\n",
        "        year_from_int = int(year_from)\n",
        "        year_to_int = int(year_to)\n",
        "        return abs(year_from_int - year_to_int)\n",
        "    except (ValueError, TypeError):\n",
        "        return 0\n",
        "\n",
        "def is_published_on_same_journal(journal_a, journal_b):\n",
        "    \"\"\"Check if two journals are the same\"\"\"\n",
        "    if journal_a == journal_b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def count_common_words(text_a, text_b):\n",
        "    \"\"\"Count common words between two text sequences\"\"\"\n",
        "    if text_a is None or text_b is None:\n",
        "        return 0\n",
        "    else:\n",
        "        return len(set(text_a).intersection(set(text_b)))"
      ],
      "metadata": {
        "id": "H_RUBN_7RlnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register UDFs with proper return types\n",
        "count_common_words_udf = udf(count_common_words, IntegerType())\n",
        "is_same_journal_udf = udf(is_published_on_same_journal, IntegerType())\n",
        "publication_year_difference_udf = udf(get_publication_year_difference, IntegerType())\n",
        "to_double_udf = udf(lambda i: 1.0 if i == 1 else 0.0, DoubleType())"
      ],
      "metadata": {
        "id": "Ckc7ZtOeSdkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_info_dataframe(filename):\n",
        "    column_names = [\"srcId\", \"year\", \"title\", \"authors\", \"journal\", \"abstract\"]\n",
        "    return spark.read.option(\"header\", \"false\").csv(filename).toDF(*column_names)"
      ],
      "metadata": {
        "id": "aQNqkR0DSoyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess the data\n",
        "def pre_process(dataframe):\n",
        "    # Create tokenizers\n",
        "    abstract_tokenizer = Tokenizer(inputCol=\"abstract\", outputCol=\"abstract_tokens_raw\")\n",
        "    title_tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_tokens_raw\")\n",
        "\n",
        "    # Create stopword removers\n",
        "    abstract_stop_words_remover = StopWordsRemover(\n",
        "        inputCol=\"abstract_tokens_raw\",\n",
        "        outputCol=\"abstract_tokens_clean\"\n",
        "    )\n",
        "    title_stop_words_remover = StopWordsRemover(\n",
        "        inputCol=\"title_tokens_raw\",\n",
        "        outputCol=\"title_tokens_clean\"\n",
        "    )\n",
        "\n",
        "    # Fill NA values and split authors\n",
        "    transformed_dataframe = dataframe.na.fill({\n",
        "        \"abstract\": \"\",\n",
        "        \"title\": \"\",\n",
        "        \"authors\": \"\",\n",
        "        \"journal\": \"\"\n",
        "    })\n",
        "    transformed_dataframe = transformed_dataframe.withColumn(\n",
        "        \"authors_tokens_raw\",\n",
        "        split(col(\"authors\"), \",\")\n",
        "    )\n",
        "\n",
        "    # Set up pipeline stages\n",
        "    stages = [\n",
        "        abstract_tokenizer,\n",
        "        abstract_stop_words_remover,\n",
        "        title_tokenizer,\n",
        "        title_stop_words_remover\n",
        "    ]\n",
        "\n",
        "    # Build the pipeline\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "\n",
        "    # Run the pipeline\n",
        "    return pipeline.fit(transformed_dataframe).transform(transformed_dataframe)"
      ],
      "metadata": {
        "id": "BpfgFwZZSjgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load training data\n",
        "def get_training_dataframe(filename):\n",
        "    # Read text file and map to DataFrame\n",
        "    return spark.read.text(filename).rdd \\\n",
        "        .map(lambda row: row.value.split(\" \")) \\\n",
        "        .map(lambda fields: (fields[0], fields[1], int(fields[2]))) \\\n",
        "        .toDF([\"srcId\", \"dstId\", \"label\"])"
      ],
      "metadata": {
        "id": "mxYo5kq-Sp4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load testing data\n",
        "def get_testing_dataframe(filename):\n",
        "    # Read text file and map to DataFrame\n",
        "    return spark.read.text(filename).rdd \\\n",
        "        .map(lambda row: row.value.split(\" \")) \\\n",
        "        .map(lambda fields: (fields[0], fields[1])) \\\n",
        "        .toDF([\"srcId\", \"dstId\"])"
      ],
      "metadata": {
        "id": "nyZYXHqfSxKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ground_truth_dataframe(filename):\n",
        "    # Read text file and map to DataFrame\n",
        "    return spark.read.text(filename).rdd \\\n",
        "        .map(lambda row: row.value.split(\"\\t\")) \\\n",
        "        .map(lambda fields: (fields[0], fields[1])) \\\n",
        "        .toDF([\"srcId\", \"dstId\"])"
      ],
      "metadata": {
        "id": "TpXuSYi3Szer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add labels to test data\n",
        "def add_labels_to_test_dataframe(testing_dataframe, ground_truth_dataframe):\n",
        "    # Join test data with ground truth to get labels\n",
        "    return testing_dataframe.alias(\"a\").join(\n",
        "        ground_truth_dataframe.alias(\"b\"),\n",
        "        (col(\"a.srcId\") == col(\"b.srcId\")) & (col(\"a.dstId\") == col(\"b.dstId\")),\n",
        "        \"left\"\n",
        "    ).withColumn(\n",
        "        \"label\",\n",
        "        when(col(\"b.srcId\").isNull(), 0).otherwise(1)\n",
        "    ).drop(\n",
        "        col(\"b.srcId\")\n",
        "    ).drop(\n",
        "        col(\"b.dstId\")\n",
        "    )"
      ],
      "metadata": {
        "id": "BPgftZRMS7AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to join dataframes\n",
        "def join_dataframes(training_dataframe, info_dataframe):\n",
        "    # First join to get source paper info\n",
        "    first_join = training_dataframe.alias(\"a\").join(\n",
        "        info_dataframe.alias(\"b\"),\n",
        "        col(\"a.srcId\") == col(\"b.srcId\")\n",
        "    ).select(\n",
        "        col(\"a.srcId\"),\n",
        "        col(\"a.dstId\"),\n",
        "        col(\"a.label\"),\n",
        "        col(\"b.year\"),\n",
        "        col(\"b.title_tokens_clean\"),\n",
        "        col(\"b.authors_tokens_raw\"),\n",
        "        col(\"b.journal\"),\n",
        "        col(\"b.abstract_tokens_clean\")\n",
        "    ).withColumnRenamed(\"srcId\", \"id_from\") \\\n",
        "     .withColumnRenamed(\"dstId\", \"id_to\") \\\n",
        "     .withColumnRenamed(\"year\", \"year_from\") \\\n",
        "     .withColumnRenamed(\"title_tokens_clean\", \"title_from\") \\\n",
        "     .withColumnRenamed(\"authors_tokens_raw\", \"authors_from\") \\\n",
        "     .withColumnRenamed(\"journal\", \"journal_from\") \\\n",
        "     .withColumnRenamed(\"abstract_tokens_clean\", \"abstract_from\")\n",
        "\n",
        "    # Second join to get target paper info\n",
        "    second_join = first_join.alias(\"a\").join(\n",
        "        info_dataframe.alias(\"b\"),\n",
        "        col(\"a.id_to\") == col(\"b.srcId\")\n",
        "    ).select(\n",
        "        \"id_from\", \"id_to\", \"label\",\n",
        "        \"year_from\", \"title_from\", \"authors_from\", \"journal_from\", \"abstract_from\",\n",
        "        col(\"b.year\").alias(\"year_to\"),\n",
        "        col(\"b.title_tokens_clean\").alias(\"title_to\"),\n",
        "        col(\"b.authors_tokens_raw\").alias(\"authors_to\"),\n",
        "        col(\"b.journal\").alias(\"journal_to\"),\n",
        "        col(\"b.abstract_tokens_clean\").alias(\"abstract_to\")\n",
        "    )\n",
        "\n",
        "    return second_join"
      ],
      "metadata": {
        "id": "YZueZ_T8S-YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_final_dataframe(joined_dataframe):\n",
        "    # Add feature columns\n",
        "    feature_df = joined_dataframe.withColumn(\n",
        "        \"common_title_words\",\n",
        "        count_common_words_udf(col(\"title_from\"), col(\"title_to\"))\n",
        "    ).withColumn(\n",
        "        \"common_authors\",\n",
        "        count_common_words_udf(col(\"authors_from\"), col(\"authors_to\"))\n",
        "    ).withColumn(\n",
        "        \"common_abstract_words\",\n",
        "        count_common_words_udf(col(\"abstract_from\"), col(\"abstract_to\"))\n",
        "    ).withColumn(\n",
        "        \"publication_year_difference\",\n",
        "        publication_year_difference_udf(\"year_from\", \"year_to\")\n",
        "    ).withColumn(\n",
        "        \"is_same_journal\",\n",
        "        is_same_journal_udf(col(\"journal_from\"), col(\"journal_to\"))\n",
        "    ).withColumn(\n",
        "        \"label\",\n",
        "        to_double_udf(col(\"label\"))\n",
        "    ).select(\n",
        "        \"label\",\n",
        "        \"common_title_words\",\n",
        "        \"common_authors\",\n",
        "        \"common_abstract_words\",\n",
        "        \"publication_year_difference\",\n",
        "        \"is_same_journal\"\n",
        "    )\n",
        "\n",
        "    # Create feature vector using VectorAssembler\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\n",
        "            \"common_title_words\",\n",
        "            \"common_authors\",\n",
        "            \"common_abstract_words\",\n",
        "            \"publication_year_difference\",\n",
        "            \"is_same_journal\"\n",
        "        ],\n",
        "        outputCol=\"features\"\n",
        "    )\n",
        "\n",
        "    # Apply the assembler and drop null values\n",
        "    return assembler.transform(feature_df).na.drop()"
      ],
      "metadata": {
        "id": "bJsekUSJTCOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics_svm(predictions):\n",
        "    # Count for confusion matrix and derived metrics\n",
        "    tp = predictions.filter((col(\"prediction\") == 1.0) & (col(\"label\") == 1.0)).count()\n",
        "    fp = predictions.filter((col(\"prediction\") == 1.0) & (col(\"label\") == 0.0)).count()\n",
        "    tn = predictions.filter((col(\"prediction\") == 0.0) & (col(\"label\") == 0.0)).count()\n",
        "    fn = predictions.filter((col(\"prediction\") == 0.0) & (col(\"label\") == 1.0)).count()\n",
        "\n",
        "    # Calculate metrics for positive class (class 1)\n",
        "    precision1 = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall1 = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1_score1 = 2 * precision1 * recall1 / (precision1 + recall1) if (precision1 + recall1) > 0 else 0.0\n",
        "\n",
        "    print(\"\\nMetrics for positive class:\")\n",
        "    print(f\"Precision: {precision1}\")\n",
        "    print(f\"Recall: {recall1}\")\n",
        "    print(f\"F1-Score: {f1_score1}\")\n",
        "\n",
        "    # Calculate metrics for negative class (class 0)\n",
        "    precision0 = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
        "    recall0 = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    f1_score0 = 2 * precision0 * recall0 / (precision0 + recall0) if (precision0 + recall0) > 0 else 0.0\n",
        "\n",
        "    print(\"\\nMetrics for negative class:\")\n",
        "    print(f\"Precision: {precision0}\")\n",
        "    print(f\"Recall: {recall0}\")\n",
        "    print(f\"F1-Score: {f1_score0}\")\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(f\"TP: {tp}, FP: {fp}\")\n",
        "    print(f\"FN: {fn}, TN: {tn}\")\n",
        "\n",
        "    # Overall accuracy\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    print(f\"\\nOverall Accuracy: {accuracy}\")\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "St7Iz58NTGWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics_mlp(predictions):\n",
        "    # Count for confusion matrix\n",
        "    tp = predictions.filter((col(\"prediction\") == 1.0) & (col(\"label\") == 1.0)).count()\n",
        "    fp = predictions.filter((col(\"prediction\") == 1.0) & (col(\"label\") == 0.0)).count()\n",
        "    tn = predictions.filter((col(\"prediction\") == 0.0) & (col(\"label\") == 0.0)).count()\n",
        "    fn = predictions.filter((col(\"prediction\") == 0.0) & (col(\"label\") == 1.0)).count()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "5AaiBKd4TJJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import LogisticRegressionModel\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate metrics for probability-based models using ROC Curve\n",
        "def calculate_metrics(predictions):\n",
        "    # Extract probability of class 1\n",
        "    get_prob1 = udf(lambda v: float(v[1]), DoubleType())\n",
        "    predictions = predictions.withColumn(\"prob1\", get_prob1(col(\"probability\")))\n",
        "\n",
        "    # Use built-in evaluator to get ROC AUC\n",
        "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "    roc_auc = evaluator.evaluate(predictions)\n",
        "    print(f\"Area under ROC curve: {roc_auc}\")\n",
        "\n",
        "    # Manually create thresholds between 0.0 and 1.0 with a step size (for efficiency)\n",
        "    thresholds = np.arange(0.0, 1.0, 0.1)  # 0.0 to 1.0 in 0.01 steps\n",
        "\n",
        "    best_f1_score = 0.0\n",
        "    optimal_threshold = 0.5\n",
        "\n",
        "    f1_scores = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        # Apply threshold to get predictions\n",
        "        pred_thresh = predictions.withColumn(\n",
        "            \"prediction_optimal\", when(col(\"prob1\") >= threshold, 1.0).otherwise(0.0)\n",
        "        )\n",
        "\n",
        "        tp = pred_thresh.filter((col(\"prediction_optimal\") == 1.0) & (col(\"label\") == 1.0)).count()\n",
        "        fp = pred_thresh.filter((col(\"prediction_optimal\") == 1.0) & (col(\"label\") == 0.0)).count()\n",
        "        fn = pred_thresh.filter((col(\"prediction_optimal\") == 0.0) & (col(\"label\") == 1.0)).count()\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "        f1_scores.append((threshold, f1))  # Store F1 score for this threshold\n",
        "\n",
        "        if f1 > best_f1_score:\n",
        "            best_f1_score = f1\n",
        "            optimal_threshold = threshold\n",
        "\n",
        "    # Plot the F1 scores to see the best threshold\n",
        "    best_f1_threshold = max(f1_scores, key=lambda x: x[1])\n",
        "    print(f\"\\nOptimal threshold based on best F1-Score: {best_f1_threshold[0]}\")\n",
        "    print(f\"Best F1-Score: {best_f1_threshold[1]}\")\n",
        "\n",
        "    # Final metrics using optimal threshold\n",
        "    final_pred = predictions.withColumn(\n",
        "        \"prediction_optimal\", when(col(\"prob1\") >= best_f1_threshold[0], 1.0).otherwise(0.0)\n",
        "    )\n",
        "\n",
        "    tp = final_pred.filter((col(\"prediction_optimal\") == 1.0) & (col(\"label\") == 1.0)).count()\n",
        "    fp = final_pred.filter((col(\"prediction_optimal\") == 1.0) & (col(\"label\") == 0.0)).count()\n",
        "    tn = final_pred.filter((col(\"prediction_optimal\") == 0.0) & (col(\"label\") == 0.0)).count()\n",
        "    fn = final_pred.filter((col(\"prediction_optimal\") == 0.0) & (col(\"label\") == 1.0)).count()\n",
        "\n",
        "    precision1 = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall1 = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "\n",
        "    print(\"\\nMetrics for positive class at optimal threshold:\")\n",
        "    print(f\"Precision: {precision1}\")\n",
        "    print(f\"Recall: {recall1}\")\n",
        "\n",
        "    precision0 = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
        "    recall0 = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    f1_score0 = 2 * precision0 * recall0 / (precision0 + recall0) if (precision0 + recall0) > 0 else 0.0\n",
        "\n",
        "    print(\"\\nMetrics for negative class at optimal threshold:\")\n",
        "    print(f\"Precision: {precision0}\")\n",
        "    print(f\"Recall: {recall0}\")\n",
        "    print(f\"F1-Score: {f1_score0}\")\n",
        "\n",
        "    print(\"\\nConfusion Matrix at optimal threshold:\")\n",
        "    print(f\"TP: {tp}, FP: {fp}\")\n",
        "    print(f\"FN: {fn}, TN: {tn}\")\n",
        "\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    print(f\"\\nOverall Accuracy at optimal threshold: {accuracy}\")"
      ],
      "metadata": {
        "id": "bFZL_37yTLdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate MLP layer configurations\n",
        "def generate_layer_configs(input_size, output_size=2, max_hidden_layers=3, max_neurons=16, step=4):\n",
        "    configs = []\n",
        "\n",
        "    for num_hidden_layers in range(1, max_hidden_layers + 1):\n",
        "        neuron_options = list(range(step, max_neurons + 1, step))\n",
        "\n",
        "        # Generate a limited number of configurations for each layer count\n",
        "        for i in range(min(5, len(neuron_options)**num_hidden_layers)):\n",
        "            hidden_layers = []\n",
        "            for j in range(num_hidden_layers):\n",
        "                # Simple way to get different layer sizes\n",
        "                size = neuron_options[(i+j) % len(neuron_options)]\n",
        "                hidden_layers.append(size)\n",
        "\n",
        "            configs.append([input_size] + hidden_layers + [output_size])\n",
        "\n",
        "    return configs"
      ],
      "metadata": {
        "id": "txydOq2-TTia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main(model_number=3):\n",
        "    print(\"Retrieving DataFrames...\")\n",
        "\n",
        "    # Load and process data\n",
        "    info_dataframe = pre_process(\n",
        "        get_info_dataframe(NODE_INFO_FILENAME).sample(withReplacement=False, fraction=INFO_DATAFRAME_PORTION, seed=12345)\n",
        "    )\n",
        "    training_dataframe = get_training_dataframe(TRAINING_SET_FILENAME)\n",
        "    testing_dataframe = get_testing_dataframe(TESTING_SET_FILENAME)\n",
        "    ground_truth_dataframe = get_ground_truth_dataframe(GROUND_TRUTH_FILENAME)\n",
        "    labeled_testing_dataframe = add_labels_to_test_dataframe(testing_dataframe, ground_truth_dataframe)\n",
        "\n",
        "    print(\"Joining DataFrames...\")\n",
        "    joined_train_dataframe = join_dataframes(training_dataframe, info_dataframe)\n",
        "    joined_test_dataframe = join_dataframes(labeled_testing_dataframe, info_dataframe)\n",
        "\n",
        "    final_train_dataframe = get_final_dataframe(joined_train_dataframe)\n",
        "    final_test_dataframe = get_final_dataframe(joined_test_dataframe)\n",
        "\n",
        "    # Run different models based on model_number\n",
        "    if model_number == 0:\n",
        "        print(\"Running Logistic Regression classification...\\n\")\n",
        "        model = LogisticRegression(\n",
        "            featuresCol=\"features\",\n",
        "            labelCol=\"label\",\n",
        "            predictionCol=\"prediction\",\n",
        "            probabilityCol=\"probability\",\n",
        "            maxIter=LOGISTIC_REGRESSION_ITERATIONS\n",
        "        )\n",
        "\n",
        "        predictions = model.fit(final_train_dataframe).transform(final_test_dataframe)\n",
        "        print(\"Calculating metrics...\\n\")\n",
        "        calculate_metrics(predictions)\n",
        "\n",
        "    elif model_number == 1:\n",
        "        print(\"Running Naive Bayes classification...\\n\")\n",
        "        model = NaiveBayes(\n",
        "            featuresCol=\"features\",\n",
        "            labelCol=\"label\",\n",
        "            predictionCol=\"prediction\",\n",
        "            probabilityCol=\"probability\",\n",
        "            modelType=\"multinomial\",\n",
        "            smoothing=1.0\n",
        "        )\n",
        "\n",
        "        predictions = model.fit(final_train_dataframe).transform(final_test_dataframe)\n",
        "        print(\"Calculating metrics...\\n\")\n",
        "        calculate_metrics(predictions)\n",
        "\n",
        "    elif model_number == 2:\n",
        "        print(\"Running Linear SVM classification...\\n\")\n",
        "        model = LinearSVC(\n",
        "            featuresCol=\"features\",\n",
        "            labelCol=\"label\",\n",
        "            predictionCol=\"prediction\",\n",
        "            maxIter=10,\n",
        "            regParam=0.1,\n",
        "            tol=1e-4\n",
        "        )\n",
        "\n",
        "        predictions = model.fit(final_train_dataframe).transform(final_test_dataframe)\n",
        "        print(\"Calculating metrics...\\n\")\n",
        "        calculate_metrics_svm(predictions)\n",
        "\n",
        "    elif model_number == 3:\n",
        "        print(\"Running Gradient Boosted Trees classification...\\n\")\n",
        "        model = GBTClassifier(\n",
        "            featuresCol=\"features\",\n",
        "            labelCol=\"label\",\n",
        "            predictionCol=\"prediction\",\n",
        "            maxIter=4,\n",
        "            maxDepth=3,\n",
        "            stepSize=0.1,\n",
        "            maxBins=32,\n",
        "            minInstancesPerNode=1,\n",
        "            minInfoGain=0.0,\n",
        "            subsamplingRate=0.8,\n",
        "            seed=1234\n",
        "        )\n",
        "\n",
        "        predictions = model.fit(final_train_dataframe).transform(final_test_dataframe)\n",
        "        print(\"Calculating metrics...\\n\")\n",
        "        calculate_metrics_svm(predictions)\n",
        "\n",
        "    elif model_number == 4:\n",
        "        print(\"Running Decision Tree classification with dynamic maxDepth tuning...\\n\")\n",
        "\n",
        "        model = DecisionTreeClassifier(\n",
        "            featuresCol=\"features\",\n",
        "            labelCol=\"label\",\n",
        "            predictionCol=\"prediction\"\n",
        "        )\n",
        "\n",
        "        evaluator = BinaryClassificationEvaluator(\n",
        "            labelCol=\"label\",\n",
        "            rawPredictionCol=\"prediction\"\n",
        "        )\n",
        "\n",
        "        param_grid = ParamGridBuilder() \\\n",
        "            .addGrid(model.maxDepth, [5, 10, 15, 20]) \\\n",
        "            .build()\n",
        "\n",
        "        cross_validator = CrossValidator(\n",
        "            estimator=model,\n",
        "            estimatorParamMaps=param_grid,\n",
        "            evaluator=evaluator,\n",
        "            numFolds=5\n",
        "        )\n",
        "\n",
        "        print(\"Running Cross-Validation for optimal maxDepth...\\n\")\n",
        "        cv_model = cross_validator.fit(final_train_dataframe)\n",
        "\n",
        "        best_model = cv_model.bestModel\n",
        "        predictions = best_model.transform(final_test_dataframe)\n",
        "\n",
        "        best_max_depth = best_model.getMaxDepth()\n",
        "        print(f\"Best maxDepth: {best_max_depth}\")\n",
        "\n",
        "        print(\"Calculating metrics...\\n\")\n",
        "        calculate_metrics_svm(predictions)\n",
        "\n",
        "    elif model_number == 5:\n",
        "        print(\"Running Random Forest classification with dynamic maxDepth tuning...\\n\")\n",
        "\n",
        "        model = RandomForestClassifier(\n",
        "            featuresCol=\"features\",\n",
        "            labelCol=\"label\",\n",
        "            predictionCol=\"prediction\"\n",
        "        )\n",
        "\n",
        "        evaluator = BinaryClassificationEvaluator(\n",
        "            labelCol=\"label\",\n",
        "            rawPredictionCol=\"prediction\"\n",
        "        )\n",
        "\n",
        "        param_grid = ParamGridBuilder() \\\n",
        "            .addGrid(model.maxDepth, [5, 10, 15, 20]) \\\n",
        "            .build()\n",
        "\n",
        "        cross_validator = CrossValidator(\n",
        "            estimator=model,\n",
        "            estimatorParamMaps=param_grid,\n",
        "            evaluator=evaluator,\n",
        "            numFolds=5\n",
        "        )\n",
        "\n",
        "        print(\"Running Cross-Validation for optimal maxDepth...\\n\")\n",
        "        cv_model = cross_validator.fit(final_train_dataframe)\n",
        "\n",
        "        best_model = cv_model.bestModel\n",
        "        predictions = best_model.transform(final_test_dataframe)\n",
        "\n",
        "        best_max_depth = best_model.getMaxDepth()\n",
        "        print(f\"Best maxDepth: {best_max_depth}\")\n",
        "\n",
        "        print(\"Calculating metrics...\\n\")\n",
        "        calculate_metrics_svm(predictions)\n",
        "\n",
        "    else:\n",
        "        print(\"Running Multilayer Perceptron classification ...\")\n",
        "        input_size = len(final_train_dataframe.select(\"features\").first()[0])\n",
        "        layer_options = generate_layer_configs(input_size)\n",
        "\n",
        "        best_acc = 0.0\n",
        "        best_layers = []\n",
        "        best_prediction_df = None\n",
        "\n",
        "        for layer in layer_options:\n",
        "            mlp = MultilayerPerceptronClassifier(\n",
        "                featuresCol=\"features\",\n",
        "                labelCol=\"label\",\n",
        "                predictionCol=\"prediction\",\n",
        "                layers=layer,\n",
        "                maxIter=100,\n",
        "                blockSize=128,\n",
        "                seed=1234\n",
        "            )\n",
        "\n",
        "            mod = mlp.fit(final_train_dataframe)\n",
        "            preds = mod.transform(final_test_dataframe)\n",
        "            accuracy = calculate_metrics_mlp(preds)\n",
        "\n",
        "            if accuracy > best_acc:\n",
        "                best_acc = accuracy\n",
        "                best_layers = layer\n",
        "                best_prediction_df = preds\n",
        "\n",
        "        print(f\"Best MLP layers: {best_layers}\")\n",
        "        print(\"Calculating metrics for best MLP model...\\n\")\n",
        "        calculate_metrics_svm(best_prediction_df)\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"citation-search\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce noise\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Run the main function with model_number=3 (GBT Classifier)\n",
        "main(0)\n",
        "\n",
        "# Stop Spark session when done\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ec6OMHbTW8L",
        "outputId": "044db134-738f-4970-e249-e6745f9752d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving DataFrames...\n",
            "Joining DataFrames...\n",
            "Running Logistic Regression classification...\n",
            "\n",
            "Calculating metrics...\n",
            "\n",
            "Area under ROC curve: 0.8348714004955994\n",
            "\n",
            "Optimal threshold based on best F1-Score: 0.4\n",
            "Best F1-Score: 0.7837837837837838\n",
            "\n",
            "Metrics for positive class at optimal threshold:\n",
            "Precision: 0.7107843137254902\n",
            "Recall: 0.8734939759036144\n",
            "\n",
            "Metrics for negative class at optimal threshold:\n",
            "Precision: 0.7961165048543689\n",
            "Recall: 0.5815602836879432\n",
            "F1-Score: 0.6721311475409836\n",
            "\n",
            "Confusion Matrix at optimal threshold:\n",
            "TP: 145, FP: 59\n",
            "FN: 21, TN: 82\n",
            "\n",
            "Overall Accuracy at optimal threshold: 0.739413680781759\n"
          ]
        }
      ]
    }
  ]
}